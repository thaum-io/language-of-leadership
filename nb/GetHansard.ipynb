{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# Add repo\n",
    "git_dir = os.path.abspath('../')\n",
    "sys.path.append(os.path.join(git_dir, 'lib', 'utils') )\n",
    "\n",
    "# Define data output path\n",
    "data_output_path = os.path.join(git_dir, 'data', 'remotes', 'hansard-urls')\n",
    "\n",
    "# Make output dir\n",
    "if not os.path.exists(data_output_path):\n",
    "    os.makedirs(data_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gets the URLS for Downloading the Hansard for a particular year\n",
    "\n",
    "Most of the work was done here, Thanks Tim Sherratt, you're amazing! (https://timsherratt.org)\n",
    "https://github.com/GLAM-Workbench/australian-commonwealth-hansard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "import arrow\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "s = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[ 502, 503, 504 ])\n",
    "s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "s.mount('http://', HTTPAdapter(max_retries=retries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLS = {\n",
    "        'hofreps': (\n",
    "            'http://parlinfo.aph.gov.au/parlInfo/search/summary/summary.w3p;'\n",
    "            'adv=yes;orderBy=date-eLast;page={page};'\n",
    "            'query={query}%20Dataset%3Ahansardr,hansardr80;resCount=100'),\n",
    "        'senate': (\n",
    "            'http://parlinfo.aph.gov.au/parlInfo/search/summary/summary.w3p;'\n",
    "            'adv=yes;orderBy=date-eLast;page={page};'\n",
    "            'query={query}%20Dataset%3Ahansards,hansards80;resCount=100')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dictionary to csv\n",
    "# https://stackoverflow.com/questions/3086973/how-do-i-convert-this-list-of-dictionaries-to-a-csv-file\n",
    "def dict_to_csv(input_dict : dict, output_file : str,): \n",
    "    with open(output_file, 'w', newline='')  as of:\n",
    "        dict_writer = csv.DictWriter(of, input_dict[0].keys())\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(input_dict)\n",
    "\n",
    "\n",
    "def get_total_results(house, query):\n",
    "    '''\n",
    "    Get the total number of results in the search.\n",
    "    '''\n",
    "    # Insert query and page values into the ParlInfo url\n",
    "    url = URLS[house].format(query=query, page=0)\n",
    "    # Get the results page\n",
    "    response = s.get(url)\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    try:\n",
    "        # Find where the total results are given in the HTML\n",
    "        summary = soup.find('div', 'resultsSummary').contents[1].string\n",
    "        # Extract the number of results from the string\n",
    "        total = re.search(r'of (\\d+)', summary).group(1)\n",
    "    except AttributeError:\n",
    "        total = 0\n",
    "    return int(total)\n",
    "\n",
    "def get_xml_url(url):\n",
    "    '''\n",
    "    Extract the XML file url from an individual result.\n",
    "    '''\n",
    "    # Load the page for an individual result\n",
    "    response = s.get(url)\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    # Find the XML url by looking for a pattern in the href\n",
    "    xml_url = soup.find('a', href=re.compile('toc_unixml'))['href']\n",
    "    return xml_url\n",
    "\n",
    "def number_of_results(house, year):\n",
    "    '''\n",
    "    Loop through a search by house and year, finding all the urls for XML files.\n",
    "    '''\n",
    "    # Format the start and end dates\n",
    "    start_date = '01%2F01%2F{}'.format(year)\n",
    "    end_date = '31%2F12%2F{}'.format(year)\n",
    "    # Prepare the query value using the start and end dates\n",
    "    query = 'Date%3A{}%20>>%20{}'.format(start_date, end_date)\n",
    "    # Get the total results\n",
    "    total_results = get_total_results(house, query)\n",
    "    xml_urls = []\n",
    "    dates = []\n",
    "    return total_results\n",
    "    \n",
    "def harvest_year(house, year):\n",
    "    '''\n",
    "    Loop through a search by house and year, finding all the urls for XML files.\n",
    "    '''\n",
    "    # Format the start and end dates\n",
    "    start_date = '01%2F01%2F{}'.format(year)\n",
    "    end_date = '31%2F12%2F{}'.format(year)\n",
    "    # Prepare the query value using the start and end dates\n",
    "    query = 'Date%3A{}%20>>%20{}'.format(start_date, end_date)\n",
    "    # Get the total results\n",
    "    total_results = get_total_results(house, query)\n",
    "    xml_urls = []\n",
    "    dates = []\n",
    "    if total_results > 0:\n",
    "        # Calculate the number of pages in the results set\n",
    "        num_pages = int(math.ceil(total_results / 100))\n",
    "        # Loop through the page range\n",
    "        for page in range(0, num_pages + 1):\n",
    "            # Get the next page of results\n",
    "            url = URLS[house].format(query=query, page=page)\n",
    "            response = s.get(url)\n",
    "            # Parse the HTML\n",
    "            soup = BeautifulSoup(response.text)\n",
    "            # Find the list of results and loop through them\n",
    "            for result in (soup.find_all('div', 'resultContent')):\n",
    "                # Try to identify the date\n",
    "                try:\n",
    "                    date = re.search(r'Date: (\\d{2}\\/\\d{2}\\/\\d{4})', result.find('div', 'sumMeta').get_text()).group(1)\n",
    "                    date = arrow.get(date, 'DD/MM/YYYY').format('YYYY-MM-DD')\n",
    "                except AttributeError:\n",
    "                    #There are some dodgy dates -- we'll just ignore them\n",
    "                    date = None\n",
    "                # If there's a date, and we haven't seen it already, we'll grab the details\n",
    "                if date and date not in dates:\n",
    "                    dates.append(date)\n",
    "                    # Get the link to the individual result page\n",
    "                    # This is where the XML file links live\n",
    "                    result_link = result.find('div', 'sumLink').a['href']\n",
    "                    # Get the XML file link from the individual record page\n",
    "                    xml_url = get_xml_url(result_link)\n",
    "                    # Save dates and links\n",
    "                    xml_urls.append({'date': date, 'url': 'https://parlinfo.aph.gov.au{}'.format(xml_url)})\n",
    "                    time.sleep(1)\n",
    "            time.sleep(1)\n",
    "    return xml_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 2018\n",
    "\n",
    "def get_and_write_urls(year : int):\n",
    "    print(\"Processing Year {}\".format(year))\n",
    "    try:\n",
    "        senate_urls = harvest_year('senate', year)\n",
    "    except:\n",
    "        senate_urls = []\n",
    "    try:\n",
    "        horeps_urls = harvest_year('hofreps', year)\n",
    "    except:\n",
    "        horeps_urls = []\n",
    "        \n",
    "    print(\"Number of results: Senate => {} House of Reps => {}\".format(len(senate_urls), len(horeps_urls)))\n",
    "\n",
    "    senate_output_file = data_output_path+\"/{}-au-hansard-senate.csv\".format(year)\n",
    "    horeps_output_file = data_output_path+\"/{}-au-hansard-hofreps.csv\".format(year)\n",
    "\n",
    "    clean_url = lambda x: x['url'].split(\";\")[0]+'\\n'\n",
    "\n",
    "    with open(senate_output_file, \"w\") as output:\n",
    "        output.writelines(map(clean_url, senate_urls))\n",
    "\n",
    "    with open(horeps_output_file, \"w\") as output:\n",
    "        output.writelines(map(clean_url, horeps_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Year 2008\n",
      "Number of results: Senate => 0 House of Reps => 69\n",
      "Processing Year 2007\n",
      "Number of results: Senate => 41 House of Reps => 0\n",
      "Processing Year 2006\n"
     ]
    }
   ],
   "source": [
    "for y in reversed(range(1979, 2009)):\n",
    "    get_and_write_urls(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env]",
   "language": "python",
   "name": "conda-env-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

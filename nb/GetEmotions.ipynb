{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add repo\n",
    "git_dir = os.path.abspath('../')\n",
    "sys.path.append(os.path.join(git_dir, 'lib', 'GoEmotions-pytorch') )\n",
    "sys.path.append(os.path.join(git_dir, 'lib', 'utils') )\n",
    "\n",
    "# Define data path\n",
    "data_path = os.path.join(git_dir, 'data', 'pm-transcripts')\n",
    "\n",
    "# Define data output path\n",
    "data_output_path = os.path.join(git_dir, 'data', 'pm-transcripts-processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from model import BertForMultiLabelClassification\n",
    "from multilabel_pipeline import MultiLabelPipeline\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "model = BertForMultiLabelClassification.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "model = model.to('cuda:0')\n",
    "\n",
    "goemotions = MultiLabelPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    threshold=0.3,\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml_cleaner import get_transcript_fname_by_id, parse_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataframe from index\n",
    "ts_path = os.path.join(data_path, 'transcripts') # path folder of where pm-transcripts are stored\n",
    "index_file_path = os.path.join(data_path, 'index.csv')\n",
    "index_df = pd.read_csv(index_file_path)\n",
    "\n",
    "# Make output dir\n",
    "if not os.path.exists(data_output_path):\n",
    "    os.makedirs(data_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts['emotions'] = [goemotions(x)[0] for x in ts['sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5865 [00:00<?, ?it/s]/home/mahasen/thaum/hackathons/govhack_2020/language-of-leadership/lib/utils/xml_cleaner.py:47: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 47 of the file /home/mahasen/thaum/hackathons/govhack_2020/language-of-leadership/lib/utils/xml_cleaner.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = bs(xml_file)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (866 > 512). Running this sequence through the model will result in indexing errors\n",
      " 24%|██▍       | 1409/5865 [00:11<00:36, 121.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 12219 in document 71 is too long\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 4022/5865 [1:33:23<28:11,  1.09it/s]  Token indices sequence length is longer than the specified maximum sequence length for this model (895 > 512). Running this sequence through the model will result in indexing errors\n",
      " 69%|██████▊   | 4023/5865 [1:33:38<2:32:33,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 22364 in document 31 is too long\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 5192/5865 [2:14:33<38:23,  3.42s/it]  Token indices sequence length is longer than the specified maximum sequence length for this model (1344 > 512). Running this sequence through the model will result in indexing errors\n",
      " 89%|████████▊ | 5193/5865 [2:14:54<1:35:58,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 11294 in document 0 is too long\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 5500/5865 [2:25:03<17:14,  2.83s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 21168 in document 19 is too long\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5865/5865 [2:36:58<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# Choose PM\n",
    "pm_name = 'Howard, John'\n",
    "\n",
    "# Get all IDs\n",
    "ts_ids = list(index_df[index_df['pm']==pm_name]['id'].astype(int))\n",
    "\n",
    "# Iterate\n",
    "for i, ts_id in enumerate(tqdm(ts_ids)):\n",
    "    f_out = os.path.join(data_output_path, str(ts_id)+'.pkl')\n",
    "    if not os.path.exists(f_out):\n",
    "        ts = parse_transcript(get_transcript_fname_by_id(ts_path, ts_id))\n",
    "        if ts is not None:\n",
    "            ts['emotions'] = []\n",
    "            for sentence_ind, sentence in enumerate(ts['sentences']):\n",
    "                tokens = tokenizer(sentence)\n",
    "                if len(tokens['input_ids'])>500:\n",
    "                    print('Sentence %d in document %d is too long'%(ts_id, sentence_ind))\n",
    "                else:\n",
    "                    ts['emotions'].append(goemotions(sentence)[0])\n",
    "\n",
    "            with open(f_out, 'wb') as f:\n",
    "                pickle.dump(ts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat using Ekman Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dfd9b5dabf10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertForMultiLabelClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmultilabel_pipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiLabelPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from model import BertForMultiLabelClassification\n",
    "from multilabel_pipeline import MultiLabelPipeline\n",
    "from pprint import pprint\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/bert-base-cased-goemotions-ekman\")\n",
    "model = BertForMultiLabelClassification.from_pretrained(\"monologg/bert-base-cased-goemotions-ekman\")\n",
    "\n",
    "goemotions = MultiLabelPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    threshold=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spacy]",
   "language": "python",
   "name": "conda-env-spacy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
